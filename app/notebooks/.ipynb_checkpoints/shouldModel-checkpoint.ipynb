{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815c87cc-602a-457f-8286-3832004e8f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_553/701452365.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kh√¥ng t√¨m th·∫•y c·ªôt 'published_date', s·ª≠ d·ª•ng ch·ªâ s·ªë ƒë·ªÉ thay th·∫ø\n",
      "# B√°o c√°o ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n d·ª±a tr√™n tin t·ª©c\n",
      "\n",
      "## T·ªïng quan th·ªã tr∆∞·ªùng\n",
      "\n",
      "- T√¢m l√Ω th·ªã tr∆∞·ªùng ƒëang r·∫•t t√≠ch c·ª±c\n",
      "- Tin t·ª©c t√≠ch c·ª±c: 53.3%\n",
      "- Tin t·ª©c ti√™u c·ª±c: 30.0%\n",
      "- Tin t·ª©c trung l·∫≠p: 16.7%\n",
      "\n",
      "## Xu h∆∞·ªõng th·ªã tr∆∞·ªùng\n",
      "\n",
      "- Tin t·ª©c t√≠ch c·ª±c ƒëang ·ªïn ƒë·ªãnh\n",
      "- Tin t·ª©c ti√™u c·ª±c ƒëang ·ªïn ƒë·ªãnh\n",
      "\n",
      "## T·ª´ kh√≥a n·ªïi b·∫≠t g·∫ßn ƒë√¢y\n",
      "\n",
      "- phi·∫øu: 17 l·∫ßn xu·∫•t hi·ªán\n",
      "- tƒÉng: 11 l·∫ßn xu·∫•t hi·ªán\n",
      "- ch·ª©ng: 9 l·∫ßn xu·∫•t hi·ªán\n",
      "- kho√°n: 9 l·∫ßn xu·∫•t hi·ªán\n",
      "- tr·∫ßn: 8 l·∫ßn xu·∫•t hi·ªán\n",
      "- k·ªãch: 4 l·∫ßn xu·∫•t hi·ªán\n",
      "- kh·ªëi: 4 l·∫ßn xu·∫•t hi·ªán\n",
      "\n",
      "## Ph√¢n t√≠ch theo ng√†nh\n",
      "\n",
      "### NƒÉng L∆∞·ª£ng\n",
      "- S·ªë tin t·ª©c: 2\n",
      "- T√≠ch c·ª±c: 100.0%\n",
      "- Ti√™u c·ª±c: 0.0%\n",
      "- Trung l·∫≠p: 0.0%\n",
      "- **Khuy·∫øn ngh·ªã:** C·∫ßn th√™m d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a ra nh·∫≠n ƒë·ªãnh ch√≠nh x√°c\n",
      "\n",
      "### H√†ng Kh√¥ng\n",
      "- S·ªë tin t·ª©c: 2\n",
      "- T√≠ch c·ª±c: 100.0%\n",
      "- Ti√™u c·ª±c: 0.0%\n",
      "- Trung l·∫≠p: 0.0%\n",
      "- **Khuy·∫øn ngh·ªã:** C·∫ßn th√™m d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a ra nh·∫≠n ƒë·ªãnh ch√≠nh x√°c\n",
      "\n",
      "### C√¥ng Ngh·ªá\n",
      "- S·ªë tin t·ª©c: 6\n",
      "- T√≠ch c·ª±c: 66.7%\n",
      "- Ti√™u c·ª±c: 0.0%\n",
      "- Trung l·∫≠p: 33.3%\n",
      "- **Khuy·∫øn ngh·ªã:** C√¢n nh·∫Øc MUA/N·∫ÆM GI·ªÆ c·ªï phi·∫øu ng√†nh c√¥ng ngh·ªá d·ª±a tr√™n tin t·ª©c t√≠ch c·ª±c\n",
      "\n",
      "### Th·ª±c Ph·∫©m\n",
      "- S·ªë tin t·ª©c: 5\n",
      "- T√≠ch c·ª±c: 40.0%\n",
      "- Ti√™u c·ª±c: 20.0%\n",
      "- Trung l·∫≠p: 40.0%\n",
      "- **Khuy·∫øn ngh·ªã:** THEO D√ïI ng√†nh th·ª±c ph·∫©m, ch∆∞a c√≥ xu h∆∞·ªõng r√µ r√†ng\n",
      "\n",
      "### B·∫•t ƒê·ªông S·∫£n\n",
      "- S·ªë tin t·ª©c: 46\n",
      "- T√≠ch c·ª±c: 32.6%\n",
      "- Ti√™u c·ª±c: 28.3%\n",
      "- Trung l·∫≠p: 39.1%\n",
      "- **Khuy·∫øn ngh·ªã:** THEO D√ïI ng√†nh b·∫•t ƒë·ªông s·∫£n, ch∆∞a c√≥ xu h∆∞·ªõng r√µ r√†ng\n",
      "\n",
      "### Ng√¢n H√†ng\n",
      "- S·ªë tin t·ª©c: 16\n",
      "- T√≠ch c·ª±c: 37.5%\n",
      "- Ti√™u c·ª±c: 37.5%\n",
      "- Trung l·∫≠p: 25.0%\n",
      "- **Khuy·∫øn ngh·ªã:** THEO D√ïI ng√†nh ng√¢n h√†ng, ch∆∞a c√≥ xu h∆∞·ªõng r√µ r√†ng\n",
      "\n",
      "### Ch·ª©ng Kho√°n\n",
      "- S·ªë tin t·ª©c: 243\n",
      "- T√≠ch c·ª±c: 38.3%\n",
      "- Ti√™u c·ª±c: 39.5%\n",
      "- Trung l·∫≠p: 22.2%\n",
      "- **Khuy·∫øn ngh·ªã:** THEO D√ïI ng√†nh ch·ª©ng kho√°n, ch∆∞a c√≥ xu h∆∞·ªõng r√µ r√†ng\n",
      "\n",
      "## K·∫øt lu·∫≠n v√† l·ªùi khuy√™n t·ªïng th·ªÉ\n",
      "\n",
      "Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n ƒëang trong tr·∫°ng th√°i T√çCH C·ª∞C. C√≥ th·ªÉ c√¢n nh·∫Øc GIA TƒÇNG v·ªã th·∫ø v√†o c√°c m√£ c√≥ tri·ªÉn v·ªçng t·ªët.\n",
      "\n",
      "**L∆∞u √Ω:** ƒê√¢y ch·ªâ l√† ph√¢n t√≠ch d·ª±a tr√™n tin t·ª©c, nh√† ƒë·∫ßu t∆∞ n√™n k·∫øt h·ª£p v·ªõi ph√¢n t√≠ch k·ªπ thu·∫≠t v√† c∆° b·∫£n kh√°c tr∆∞·ªõc khi ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒë·∫ßu t∆∞.\n",
      "\n",
      "\n",
      "ƒê√£ l∆∞u b√°o c√°o ƒë·∫ßu t∆∞ v√†o file 'bao_cao_dau_tu.md'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_553/701452365.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recent_df['date'] = recent_df['published_date'].dt.date\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# K·∫øt n·ªëi MongoDB\n",
    "mongo_uri = os.environ.get('MONGO_URI')\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client.get_database()\n",
    "collection = db['pred_news']\n",
    "\n",
    "# L·∫•y d·ªØ li·ªáu t·ª´ collection\n",
    "data = list(collection.find())\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Th√™m c·ªôt th·ªùi gian v√† chuy·ªÉn ƒë·ªïi sang datetime\n",
    "if 'published_date' in df.columns:\n",
    "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
    "    # S·∫Øp x·∫øp theo th·ªùi gian\n",
    "    df = df.sort_values('published_date')\n",
    "else:\n",
    "    print(\"Kh√¥ng t√¨m th·∫•y c·ªôt 'published_date', s·ª≠ d·ª•ng ch·ªâ s·ªë ƒë·ªÉ thay th·∫ø\")\n",
    "    df['published_date'] = pd.to_datetime('today') - pd.to_timedelta(np.arange(len(df)), 'D')\n",
    "\n",
    "# T·∫°o h√†m ph√¢n t√≠ch xu h∆∞·ªõng\n",
    "def analyze_trend(dataframe):\n",
    "    # Ph√¢n t√≠ch xu h∆∞·ªõng g·∫ßn ƒë√¢y (30 ng√†y g·∫ßn nh·∫•t ho·∫∑c t·∫•t c·∫£ n·∫øu √≠t h∆°n)\n",
    "    recent_days = 30\n",
    "    today = pd.to_datetime('today')\n",
    "    if 'published_date' in dataframe.columns:\n",
    "        recent_df = dataframe[dataframe['published_date'] >= (today - timedelta(days=recent_days))]\n",
    "        if len(recent_df) < 5:  # N·∫øu kh√¥ng ƒë·ªß d·ªØ li·ªáu, l·∫•y 30% d·ªØ li·ªáu g·∫ßn nh·∫•t\n",
    "            recent_df = dataframe.iloc[-int(len(dataframe)*0.3):]\n",
    "    else:\n",
    "        recent_df = dataframe.iloc[-int(len(dataframe)*0.3):]\n",
    "    \n",
    "    # Ph√¢n t√≠ch c·∫£m x√∫c g·∫ßn ƒë√¢y\n",
    "    sentiment_counts = recent_df['sentiment'].value_counts()\n",
    "    sentiment_percentage = (sentiment_counts / sentiment_counts.sum() * 100).round(1)\n",
    "    \n",
    "    # Ph√¢n t√≠ch t·ª´ kh√≥a ph·ªï bi·∫øn\n",
    "    if 'title' in recent_df.columns:\n",
    "        titles = ' '.join(recent_df['title'].astype(str).fillna(''))\n",
    "        words = re.findall(r'\\b[A-Za-z√Ä-·ªπ][A-Za-z√Ä-·ªπ]+\\b', titles)\n",
    "        \n",
    "        # Lo·∫°i b·ªè c√°c t·ª´ d·ª´ng trong ti·∫øng Vi·ªát\n",
    "        vietnamese_stopwords = [\n",
    "            'v√†', 'c·ªßa', 'c√≥', 'cho', 'c√°c', 'v·ªõi', 'l√†', 'ƒë∆∞·ª£c', 'trong', 'ƒë√£', 't·∫°i', \n",
    "            't·ª´', 'theo', 'nh·ªØng', 'ƒë·ªÉ', 'kh√¥ng', 'n√†y', 'ƒë·∫øn', 'v·ªÅ', 'c√≥ th·ªÉ', 'khi',\n",
    "            's·∫Ω', 'ƒëang', 'nhi·ªÅu', 'nh∆∞', 'nƒÉm', 'tr√™n', 'nh∆∞ng', 'sau', 'ph·∫£i', 'c≈©ng',\n",
    "            'm·ªôt', 'ƒë√¢y', 'l√†m', 'hai', 'c√≤n'\n",
    "        ]\n",
    "        \n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in vietnamese_stopwords and len(word) > 2]\n",
    "        word_counts = Counter(filtered_words)\n",
    "        \n",
    "        # L·∫•y 10 t·ª´ ph·ªï bi·∫øn nh·∫•t\n",
    "        common_words = word_counts.most_common(10)\n",
    "    else:\n",
    "        common_words = []\n",
    "    \n",
    "    # Ph√¢n t√≠ch xu h∆∞·ªõng c·∫£m x√∫c theo th·ªùi gian\n",
    "    if len(recent_df) > 5 and 'published_date' in recent_df.columns:\n",
    "        # Nh√≥m theo ng√†y v√† t√≠nh t·ª∑ l·ªá c·∫£m x√∫c\n",
    "        recent_df['date'] = recent_df['published_date'].dt.date\n",
    "        sentiment_by_date = recent_df.groupby('date')['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
    "        \n",
    "        if not sentiment_by_date.empty:\n",
    "            # T√≠nh xu h∆∞·ªõng (tƒÉng/gi·∫£m) cho m·ªói lo·∫°i c·∫£m x√∫c\n",
    "            trend = {}\n",
    "            for col in sentiment_by_date.columns:\n",
    "                if len(sentiment_by_date[col]) >= 3:  # C·∫ßn √≠t nh·∫•t 3 ƒëi·ªÉm ƒë·ªÉ x√°c ƒë·ªãnh xu h∆∞·ªõng\n",
    "                    values = sentiment_by_date[col].values\n",
    "                    if np.mean(values[-3:]) > np.mean(values[:3]):\n",
    "                        trend[col] = \"tƒÉng\"\n",
    "                    elif np.mean(values[-3:]) < np.mean(values[:3]):\n",
    "                        trend[col] = \"gi·∫£m\"\n",
    "                    else:\n",
    "                        trend[col] = \"·ªïn ƒë·ªãnh\"\n",
    "                else:\n",
    "                    trend[col] = \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\"\n",
    "        else:\n",
    "            trend = {\"positive\": \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\", \"negative\": \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\", \"neutral\": \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\"}\n",
    "    else:\n",
    "        trend = {\"positive\": \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\", \"negative\": \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\", \"neutral\": \"kh√¥ng ƒë·ªß d·ªØ li·ªáu\"}\n",
    "    \n",
    "    return {\n",
    "        \"sentiment_percentage\": sentiment_percentage.to_dict(),\n",
    "        \"common_words\": common_words,\n",
    "        \"sentiment_trend\": trend\n",
    "    }\n",
    "\n",
    "# Ph√¢n t√≠ch d·ªØ li·ªáu\n",
    "analysis_result = analyze_trend(df)\n",
    "\n",
    "# T·∫°o danh s√°ch t·ª´ kh√≥a li√™n quan ƒë·∫øn c√°c c·ªï phi·∫øu/ng√†nh c·ª• th·ªÉ\n",
    "stock_keywords = {\n",
    "    \"ng√¢n h√†ng\": [\"ng√¢n h√†ng\", \"vietcombank\", \"techcombank\", \"sacombank\", \"vietinbank\", \"vcb\", \"tcb\", \"stb\"],\n",
    "    \"b·∫•t ƒë·ªông s·∫£n\": [\"b·∫•t ƒë·ªông s·∫£n\", \"ƒë·∫•t\", \"nh√†\", \"cƒÉn h·ªô\", \"vinhomes\", \"novaland\", \"ƒë·∫ßu t∆∞\", \"x√¢y d·ª±ng\"],\n",
    "    \"ch·ª©ng kho√°n\": [\"ch·ª©ng kho√°n\", \"c·ªï phi·∫øu\", \"vn-index\", \"hnx\", \"upcom\", \"ssi\", \"vnindex\"],\n",
    "    \"nƒÉng l∆∞·ª£ng\": [\"ƒëi·ªán\", \"d·∫ßu kh√≠\", \"nƒÉng l∆∞·ª£ng\", \"ƒëi·ªán l·ª±c\", \"pvn\", \"pvgas\", \"nhi·ªát ƒëi·ªán\"],\n",
    "    \"c√¥ng ngh·ªá\": [\"fpt\", \"c√¥ng ngh·ªá\", \"ph·∫ßn m·ªÅm\", \"s·ªë h√≥a\", \"vnpay\", \"vi·ªÖn th√¥ng\"],\n",
    "    \"th·ª±c ph·∫©m\": [\"masan\", \"vinamilk\", \"sabeco\", \"th·ª±c ph·∫©m\", \"ƒë·ªì u·ªëng\"],\n",
    "    \"h√†ng kh√¥ng\": [\"vietjet\", \"vietnam airlines\", \"h√†ng kh√¥ng\", \"s√¢n bay\", \"bamboo\", \"vjc\", \"hvn\", \"adb\"]\n",
    "}\n",
    "\n",
    "# H√†m ƒë∆∞a ra ph√¢n t√≠ch v√† l·ªùi khuy√™n ƒë·∫ßu t∆∞\n",
    "def generate_investment_advice(analysis_result, df):\n",
    "    sentiment_percentage = analysis_result[\"sentiment_percentage\"]\n",
    "    common_words = analysis_result[\"common_words\"]\n",
    "    sentiment_trend = analysis_result[\"sentiment_trend\"]\n",
    "    \n",
    "    # T√¨m t·ª´ kh√≥a ng√†nh xu·∫•t hi·ªán trong danh s√°ch t·ª´ kh√≥a ph·ªï bi·∫øn\n",
    "    mentioned_sectors = {}\n",
    "    for sector, keywords in stock_keywords.items():\n",
    "        mentions = 0\n",
    "        for word, count in common_words:\n",
    "            if any(keyword in word.lower() for keyword in keywords):\n",
    "                mentions += count\n",
    "        if mentions > 0:\n",
    "            mentioned_sectors[sector] = mentions\n",
    "    \n",
    "    # T√¨m c√°c b√†i b√°o v·ªõi c·∫£m x√∫c t√≠ch c·ª±c/ti√™u c·ª±c cho t·ª´ng ng√†nh\n",
    "    sector_sentiment = {}\n",
    "    for sector, keywords in stock_keywords.items():\n",
    "        sector_df = df[df['title'].astype(str).str.lower().apply(\n",
    "            lambda x: any(keyword in x.lower() for keyword in keywords))]\n",
    "        \n",
    "        if len(sector_df) > 0:\n",
    "            pos_pct = len(sector_df[sector_df['sentiment'] == 'positive']) / len(sector_df) * 100\n",
    "            neg_pct = len(sector_df[sector_df['sentiment'] == 'negative']) / len(sector_df) * 100\n",
    "            neu_pct = len(sector_df[sector_df['sentiment'] == 'neutral']) / len(sector_df) * 100\n",
    "            \n",
    "            sector_sentiment[sector] = {\n",
    "                'positive': pos_pct, \n",
    "                'negative': neg_pct, \n",
    "                'neutral': neu_pct,\n",
    "                'count': len(sector_df)\n",
    "            }\n",
    "    \n",
    "    # T·∫°o b√°o c√°o ph√¢n t√≠ch\n",
    "    advice_text = \"# B√°o c√°o ph√¢n t√≠ch th·ªã tr∆∞·ªùng ch·ª©ng kho√°n d·ª±a tr√™n tin t·ª©c\\n\\n\"\n",
    "    \n",
    "    # Ph√¢n t√≠ch t·ªïng quan\n",
    "    advice_text += \"## T·ªïng quan th·ªã tr∆∞·ªùng\\n\\n\"\n",
    "    \n",
    "    # X√°c ƒë·ªãnh t√¢m l√Ω th·ªã tr∆∞·ªùng\n",
    "    if 'positive' in sentiment_percentage and 'negative' in sentiment_percentage:\n",
    "        total_sentiment_score = sentiment_percentage.get('positive', 0) - sentiment_percentage.get('negative', 0)\n",
    "        if total_sentiment_score > 20:\n",
    "            market_mood = \"T√¢m l√Ω th·ªã tr∆∞·ªùng ƒëang r·∫•t t√≠ch c·ª±c\"\n",
    "        elif total_sentiment_score > 10:\n",
    "            market_mood = \"T√¢m l√Ω th·ªã tr∆∞·ªùng t√≠ch c·ª±c\"\n",
    "        elif total_sentiment_score < -20:\n",
    "            market_mood = \"T√¢m l√Ω th·ªã tr∆∞·ªùng ƒëang r·∫•t ti√™u c·ª±c\"\n",
    "        elif total_sentiment_score < -10:\n",
    "            market_mood = \"T√¢m l√Ω th·ªã tr∆∞·ªùng ti√™u c·ª±c\"\n",
    "        else:\n",
    "            market_mood = \"T√¢m l√Ω th·ªã tr∆∞·ªùng trung l·∫≠p\"\n",
    "    else:\n",
    "        market_mood = \"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° t√¢m l√Ω th·ªã tr∆∞·ªùng\"\n",
    "    \n",
    "    advice_text += f\"- {market_mood}\\n\"\n",
    "    advice_text += f\"- Tin t·ª©c t√≠ch c·ª±c: {sentiment_percentage.get('positive', 0):.1f}%\\n\"\n",
    "    advice_text += f\"- Tin t·ª©c ti√™u c·ª±c: {sentiment_percentage.get('negative', 0):.1f}%\\n\"\n",
    "    advice_text += f\"- Tin t·ª©c trung l·∫≠p: {sentiment_percentage.get('neutral', 0):.1f}%\\n\\n\"\n",
    "    \n",
    "    # Ph√¢n t√≠ch xu h∆∞·ªõng\n",
    "    advice_text += \"## Xu h∆∞·ªõng th·ªã tr∆∞·ªùng\\n\\n\"\n",
    "    \n",
    "    if 'positive' in sentiment_trend:\n",
    "        if sentiment_trend['positive'] == \"tƒÉng\":\n",
    "            advice_text += \"- Tin t·ª©c t√≠ch c·ª±c ƒëang c√≥ xu h∆∞·ªõng tƒÉng üìà\\n\"\n",
    "        elif sentiment_trend['positive'] == \"gi·∫£m\":\n",
    "            advice_text += \"- Tin t·ª©c t√≠ch c·ª±c ƒëang c√≥ xu h∆∞·ªõng gi·∫£m üìâ\\n\"\n",
    "        else:\n",
    "            advice_text += \"- Tin t·ª©c t√≠ch c·ª±c ƒëang ·ªïn ƒë·ªãnh\\n\"\n",
    "    \n",
    "    if 'negative' in sentiment_trend:\n",
    "        if sentiment_trend['negative'] == \"tƒÉng\":\n",
    "            advice_text += \"- Tin t·ª©c ti√™u c·ª±c ƒëang c√≥ xu h∆∞·ªõng tƒÉng üìà\\n\"\n",
    "        elif sentiment_trend['negative'] == \"gi·∫£m\":\n",
    "            advice_text += \"- Tin t·ª©c ti√™u c·ª±c ƒëang c√≥ xu h∆∞·ªõng gi·∫£m üìâ\\n\"\n",
    "        else:\n",
    "            advice_text += \"- Tin t·ª©c ti√™u c·ª±c ƒëang ·ªïn ƒë·ªãnh\\n\"\n",
    "    \n",
    "    advice_text += \"\\n## T·ª´ kh√≥a n·ªïi b·∫≠t g·∫ßn ƒë√¢y\\n\\n\"\n",
    "    for word, count in common_words[:7]:\n",
    "        advice_text += f\"- {word}: {count} l·∫ßn xu·∫•t hi·ªán\\n\"\n",
    "    \n",
    "    # Ph√¢n t√≠ch theo ng√†nh\n",
    "    advice_text += \"\\n## Ph√¢n t√≠ch theo ng√†nh\\n\\n\"\n",
    "    \n",
    "    if sector_sentiment:\n",
    "        # S·∫Øp x·∫øp ng√†nh theo s·ªë l·∫ßn xu·∫•t hi·ªán\n",
    "        sorted_sectors = sorted(sector_sentiment.items(), \n",
    "                               key=lambda x: (x[1]['positive'] - x[1]['negative'], x[1]['count']), \n",
    "                               reverse=True)\n",
    "        \n",
    "        for sector, stats in sorted_sectors:\n",
    "            advice_text += f\"### {sector.title()}\\n\"\n",
    "            advice_text += f\"- S·ªë tin t·ª©c: {stats['count']}\\n\"\n",
    "            advice_text += f\"- T√≠ch c·ª±c: {stats['positive']:.1f}%\\n\"\n",
    "            advice_text += f\"- Ti√™u c·ª±c: {stats['negative']:.1f}%\\n\"\n",
    "            advice_text += f\"- Trung l·∫≠p: {stats['neutral']:.1f}%\\n\"\n",
    "            \n",
    "            # ƒê∆∞a ra l·ªùi khuy√™n\n",
    "            sentiment_balance = stats['positive'] - stats['negative']\n",
    "            if sentiment_balance > 20 and stats['count'] >= 5:\n",
    "                advice_text += f\"- **Khuy·∫øn ngh·ªã:** C√¢n nh·∫Øc MUA/N·∫ÆM GI·ªÆ c·ªï phi·∫øu ng√†nh {sector} d·ª±a tr√™n tin t·ª©c t√≠ch c·ª±c\\n\"\n",
    "            elif sentiment_balance < -20 and stats['count'] >= 5:\n",
    "                advice_text += f\"- **Khuy·∫øn ngh·ªã:** C√¢n nh·∫Øc B√ÅN/TR√ÅNH c·ªï phi·∫øu ng√†nh {sector} do tin t·ª©c ti√™u c·ª±c\\n\"\n",
    "            elif stats['count'] >= 5:\n",
    "                advice_text += f\"- **Khuy·∫øn ngh·ªã:** THEO D√ïI ng√†nh {sector}, ch∆∞a c√≥ xu h∆∞·ªõng r√µ r√†ng\\n\"\n",
    "            else:\n",
    "                advice_text += f\"- **Khuy·∫øn ngh·ªã:** C·∫ßn th√™m d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a ra nh·∫≠n ƒë·ªãnh ch√≠nh x√°c\\n\"\n",
    "            \n",
    "            advice_text += \"\\n\"\n",
    "    else:\n",
    "        advice_text += \"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch theo ng√†nh\\n\\n\"\n",
    "    \n",
    "    # K·∫øt lu·∫≠n\n",
    "    advice_text += \"## K·∫øt lu·∫≠n v√† l·ªùi khuy√™n t·ªïng th·ªÉ\\n\\n\"\n",
    "    \n",
    "    if 'positive' in sentiment_percentage and 'negative' in sentiment_percentage:\n",
    "        total_sentiment_score = sentiment_percentage.get('positive', 0) - sentiment_percentage.get('negative', 0)\n",
    "        \n",
    "        if total_sentiment_score > 30:\n",
    "            advice_text += \"Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n ƒëang trong tr·∫°ng th√°i R·∫§T T√çCH C·ª∞C. ƒê√¢y c√≥ th·ªÉ l√† th·ªùi ƒëi·ªÉm t·ªët ƒë·ªÉ xem x√©t MUA V√ÄO, ƒë·∫∑c bi·ªát l√† c√°c c·ªï phi·∫øu trong c√°c ng√†nh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p t√≠ch c·ª±c ·ªü tr√™n.\\n\\n\"\n",
    "        elif total_sentiment_score > 15:\n",
    "            advice_text += \"Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n ƒëang trong tr·∫°ng th√°i T√çCH C·ª∞C. C√≥ th·ªÉ c√¢n nh·∫Øc GIA TƒÇNG v·ªã th·∫ø v√†o c√°c m√£ c√≥ tri·ªÉn v·ªçng t·ªët.\\n\\n\"\n",
    "        elif total_sentiment_score < -30:\n",
    "            advice_text += \"Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n ƒëang trong tr·∫°ng th√°i R·∫§T TI√äU C·ª∞C. N√™n c√¢n nh·∫Øc B·∫¢O TO√ÄN V·ªêN, tr√°nh mua v√†o trong giai ƒëo·∫°n n√†y v√† c√≥ th·ªÉ xem x√©t B√ÅN b·ªõt c√°c c·ªï phi·∫øu r·ªßi ro cao.\\n\\n\"\n",
    "        elif total_sentiment_score < -15:\n",
    "            advice_text += \"Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n ƒëang trong tr·∫°ng th√°i TI√äU C·ª∞C. N√™n TH·∫¨N TR·ªåNG v√† c√≥ chi·∫øn l∆∞·ª£c b·∫£o v·ªá danh m·ª•c ƒë·∫ßu t∆∞.\\n\\n\"\n",
    "        else:\n",
    "            advice_text += \"Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n ƒëang trong tr·∫°ng th√°i TRUNG L·∫¨P. N√™n THEO D√ïI k·ªπ th·ªã tr∆∞·ªùng v√† ch·ªâ giao d·ªãch khi c√≥ t√≠n hi·ªáu r√µ r√†ng.\\n\\n\"\n",
    "    else:\n",
    "        advice_text += \"Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n t·ªïng th·ªÉ.\\n\\n\"\n",
    "    \n",
    "    advice_text += \"**L∆∞u √Ω:** ƒê√¢y ch·ªâ l√† ph√¢n t√≠ch d·ª±a tr√™n tin t·ª©c, nh√† ƒë·∫ßu t∆∞ n√™n k·∫øt h·ª£p v·ªõi ph√¢n t√≠ch k·ªπ thu·∫≠t v√† c∆° b·∫£n kh√°c tr∆∞·ªõc khi ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒë·∫ßu t∆∞.\\n\"\n",
    "    \n",
    "    return advice_text\n",
    "\n",
    "# T·∫°o l·ªùi khuy√™n ƒë·∫ßu t∆∞\n",
    "investment_advice = generate_investment_advice(analysis_result, df)\n",
    "\n",
    "# Hi·ªÉn th·ªã l·ªùi khuy√™n\n",
    "print(investment_advice)\n",
    "\n",
    "# L∆∞u b√°o c√°o ra file\n",
    "with open('bao_cao_dau_tu.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(investment_advice)\n",
    "\n",
    "print(\"\\nƒê√£ l∆∞u b√°o c√°o ƒë·∫ßu t∆∞ v√†o file 'bao_cao_dau_tu.md'\")\n",
    "\n",
    "# T·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch t√¢m l√Ω th·ªã tr∆∞·ªùng theo ng√†nh\n",
    "if 'sector_sentiment' in locals() and len(sector_sentiment) > 0:\n",
    "    sectors = list(sector_sentiment.keys())\n",
    "    positive_values = [sector_sentiment[s]['positive'] for s in sectors]\n",
    "    negative_values = [sector_sentiment[s]['negative'] for s in sectors]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    x = np.arange(len(sectors))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, positive_values, width, label='T√≠ch c·ª±c', color='green')\n",
    "    plt.bar(x + width/2, negative_values, width, label='Ti√™u c·ª±c', color='red')\n",
    "    \n",
    "    plt.xlabel('Ng√†nh')\n",
    "    plt.ylabel('T·ª∑ l·ªá (%)')\n",
    "    plt.title('T√¢m l√Ω th·ªã tr∆∞·ªùng theo ng√†nh')\n",
    "    plt.xticks(x, [sector.title() for sector in sectors], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('phan_tich_nganh.png', dpi=300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
